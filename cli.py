#!/usr/bin/env python3
"""
Multi-Agent Research System CLI - ENHANCED VERSION
Clean Click-based command line interface with advanced research capabilities
"""

import click
import subprocess
import sys
import time
import re
import json
import requests
import textwrap
from pathlib import Path
from typing import Optional, List

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

# CLI Configuration
CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])


def echo_success(message: str):
    """Print success message"""
    click.echo(click.style(f"‚úÖ {message}", fg='green'))


def echo_error(message: str):
    """Print error message"""
    click.echo(click.style(f"‚ùå {message}", fg='red'))


def echo_warning(message: str):
    """Print warning message"""
    click.echo(click.style(f"‚ö†Ô∏è {message}", fg='yellow'))


def echo_info(message: str):
    """Print info message"""
    click.echo(click.style(f"‚ÑπÔ∏è {message}", fg='blue'))


def echo_step(step: str, message: str):
    """Print step message"""
    click.echo(click.style(f"{step} {message}", fg='white', bold=True))


def echo_header(message: str):
    """Print a header"""
    click.echo("\n" + "=" * 60)
    click.echo(click.style(message, fg='cyan', bold=True))
    click.echo("=" * 60)


class SystemManager:
    """Core system management functionality - FIXED VERSION"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.compose_file = self.project_root / "docker-compose.yml"
        self.env_file = self.project_root / ".env"

    def check_prerequisites(self) -> bool:
        """Check system prerequisites"""
        echo_step("üîß", "Checking prerequisites...")

        # Check Docker
        if not self._check_command(["docker", "--version"]):
            echo_error("Docker not found. Please install Docker.")
            return False

        # Check Docker Compose
        if not self._check_command(["docker-compose", "--version"]):
            echo_error("Docker Compose not found. Please install Docker Compose.")
            return False

        # Check compose file
        if not self.compose_file.exists():
            echo_error(f"Docker compose file not found: {self.compose_file}")
            return False

        echo_success("All prerequisites met")
        return True

    def _check_command(self, cmd: List[str]) -> bool:
        """Check if a command exists"""
        try:
            result = subprocess.run(cmd, capture_output=True, timeout=10)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False

    def setup_environment(self) -> bool:
        """Setup environment file"""
        echo_step("üìù", "Checking environment...")

        if not self.env_file.exists():
            env_defaults = self.project_root / ".env.defaults"
            if env_defaults.exists():
                with open(env_defaults, 'r') as src, open(self.env_file, 'w') as dst:
                    dst.write(src.read())
                echo_success("Created .env from .env.defaults")
            else:
                echo_error("No .env or .env.defaults found")
                return False
        else:
            echo_info(".env file exists")

        return True

    def check_sources(self):
        """Check sources directory - no creation, just report"""
        echo_step("üìÅ", "Checking sources directory...")

        sources_dir = self.project_root / "sources"

        if not sources_dir.exists():
            echo_warning("Sources directory not found")
            echo_info("Create sources/ and add PDF files for document ingestion")
            return

        pdf_files = list(sources_dir.glob("*.pdf"))

        if pdf_files:
            echo_success(f"Found {len(pdf_files)} PDF files:")
            for i, pdf in enumerate(pdf_files[:5], 1):
                size_kb = pdf.stat().st_size // 1024
                click.echo(f"   {i}. {pdf.name} ({size_kb}KB)")
            if len(pdf_files) > 5:
                click.echo(f"   ... and {len(pdf_files) - 5} more")
        else:
            echo_info("No PDF files found in sources/")
            echo_info("Add PDF files to sources/ for document processing")

    def run_compose_command(self, cmd: List[str], timeout: int = 300) -> bool:
        """Run a docker-compose command with improved timeout handling"""
        try:
            full_cmd = ["docker-compose"] + cmd
            echo_info(f"Running: {' '.join(full_cmd)}")

            # For 'up' commands, don't capture output to avoid hanging
            if 'up' in cmd:
                echo_info(f"Starting services (timeout: {timeout}s)...")
                result = subprocess.run(
                    full_cmd,
                    cwd=self.project_root,
                    timeout=timeout,
                    # Don't capture output for 'up' commands - let them stream
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
            else:
                result = subprocess.run(
                    full_cmd,
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=timeout
                )

            if result.returncode == 0:
                echo_success("Command completed successfully")
                return True
            else:
                echo_warning(f"Command returned exit code: {result.returncode}")
                return self._check_if_services_actually_started(cmd)

        except subprocess.TimeoutExpired:
            echo_warning(f"Command timed out after {timeout}s, checking if services started...")
            # Don't treat timeout as failure - services might still be starting
            return self._check_if_services_actually_started(cmd)
        except Exception as e:
            echo_error(f"Command error: {e}")
            return False

    def _check_if_services_actually_started(self, cmd: List[str]) -> bool:
        """Check if services actually started despite timeout"""
        if 'up' not in cmd:
            return False

        try:
            # Give services a moment to start
            echo_info("Verifying service status...")
            time.sleep(10)

            # Check if containers are running
            result = subprocess.run(
                ["docker-compose", "ps"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                running_services = []
                for line in result.stdout.split('\n'):
                    if 'Up' in line and 'Exit' not in line:
                        # Extract service name from docker-compose ps output
                        parts = line.split()
                        if len(parts) > 0:
                            service_name = parts[0].split('_')[-1].split('-')[0]
                            if service_name not in running_services:
                                running_services.append(service_name)

                if running_services:
                    echo_success(f"Services are running: {', '.join(running_services)}")
                    return True
                else:
                    echo_warning("No services appear to be running")
                    return False
            else:
                return False

        except Exception as e:
            echo_warning(f"Could not verify service status: {e}")
            return False

    def wait_for_service(self, service: str, max_wait: int = 240) -> bool:
        """Wait for a service to become healthy with better detection"""
        echo_step("‚è≥", f"Waiting for {service} to be ready...")

        start_time = time.time()
        check_interval = 10  # Check every 10 seconds

        while time.time() - start_time < max_wait:
            try:
                # More specific health checks per service
                if self._check_service_health(service):
                    echo_success(f"{service} is ready")
                    return True

                elapsed = int(time.time() - start_time)
                if elapsed % 30 == 0 and elapsed > 0:  # Report every 30 seconds
                    echo_info(f"Still waiting for {service}... ({elapsed}s/{max_wait}s)")

                time.sleep(check_interval)

            except Exception as e:
                echo_warning(f"Error checking {service}: {e}")
                time.sleep(check_interval)

        echo_warning(f"Timeout waiting for {service} after {max_wait}s (service may still be starting)")
        return False

    def _check_service_health(self, service: str) -> bool:
        """Improved service health checks"""
        try:
            if service == "neo4j":
                return self._check_neo4j_health()
            elif service == "mongodb":
                return self._check_mongodb_health()
            elif service == "chromadb":
                return self._check_chromadb_health()
            elif service == "api":
                return self._check_api_health()
            else:
                # Fallback: check if container is running
                result = subprocess.run(
                    ["docker-compose", "ps", service],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                return result.returncode == 0 and ("Up" in result.stdout)

        except Exception:
            return False

    def _check_neo4j_health(self) -> bool:
        """Check Neo4j health"""
        try:
            response = requests.get("http://localhost:7474", timeout=5)
            return response.status_code == 200
        except:
            return False

    def _check_mongodb_health(self) -> bool:
        """Check MongoDB health"""
        try:
            # Try to connect to MongoDB
            from pymongo import MongoClient
            client = MongoClient("mongodb://user:password@localhost:27017/", serverSelectionTimeoutMS=5000)
            client.server_info()
            return True
        except:
            return False

    def _check_chromadb_health(self) -> bool:
        """Check ChromaDB health"""
        try:
            response = requests.get("http://localhost:8001/api/v1/heartbeat", timeout=5)
            return response.status_code == 200
        except:
            try:
                # Alternative endpoint
                response = requests.get("http://localhost:8001", timeout=5)
                return response.status_code == 200
            except:
                return False

    def _check_api_health(self) -> bool:
        """Check API health"""
        try:
            response = requests.get("http://localhost:8000/api/v1/status", timeout=5)
            return response.status_code == 200
        except:
            return False

    def show_logs(self, service: Optional[str] = None, lines: int = 20):
        """Show service logs"""
        cmd = ["logs", "--tail", str(lines)]
        if service:
            cmd.append(service)

        try:
            full_cmd = ["docker-compose"] + cmd
            result = subprocess.run(
                full_cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                if service:
                    echo_step("üìã", f"Recent {service} logs:")
                else:
                    echo_step("üìã", "Recent system logs:")

                for line in result.stdout.split('\n'):
                    if line.strip():
                        click.echo(f"   {line}")
            else:
                echo_error("Failed to get logs")

        except Exception as e:
            echo_error(f"Error getting logs: {e}")


def verify_system_with_http_calls(quick: bool = False) -> bool:
    """Verify system is working with actual HTTP calls"""
    services_to_check = [
        ("Neo4j Browser", "http://localhost:7474", 5),
        ("MongoDB Express", "http://localhost:8081", 10),
        ("ChromaDB", "http://localhost:8001", 5),
        ("API Status", "http://localhost:8000/api/v1/status", 10)
    ]

    if quick:
        # Only check API in quick mode
        services_to_check = [("API Status", "http://localhost:8000/api/v1/status", 5)]

    healthy_count = 0

    echo_info("Verifying services are responding...")
    for name, url, timeout in services_to_check:
        try:
            response = requests.get(url, timeout=timeout)
            if response.status_code == 200:
                echo_success(f"{name} is responding")
                healthy_count += 1
            else:
                echo_warning(f"{name} returned status {response.status_code}")
        except Exception as e:
            echo_warning(f"{name} not responding yet")

    return healthy_count > 0  # At least one service working


def show_startup_summary():
    """Show startup summary"""
    echo_step("üìã", "System Summary:")

    # Show access points
    click.echo("\nüåê Access Points:")
    access_points = [
        ("API", "http://localhost:8000"),
        ("API Status", "http://localhost:8000/api/v1/status"),
        ("Neo4j Browser", "http://localhost:7474 (neo4j/password)"),
        ("MongoDB Express", "http://localhost:8081"),
        ("ChromaDB", "http://localhost:8001")
    ]

    for name, url in access_points:
        click.echo(f"   ‚Ä¢ {name}: {click.style(url, fg='blue')}")


# Create the main CLI group
@click.group(context_settings=CONTEXT_SETTINGS)
@click.option('--verbose', '-v', is_flag=True, help='Enable verbose output')
@click.option('--project-dir', default='.', help='Project directory path')
@click.pass_context
def cli(ctx, verbose, project_dir):
    """üöÄ Multi-Agent Research System CLI

    Command-line interface for managing your multi-agent research system.
    """
    ctx.ensure_object(dict)
    ctx.obj['verbose'] = verbose
    ctx.obj['manager'] = SystemManager(project_dir)


@cli.command()
@click.option('--timeout', '-t', default=600, help='Maximum startup timeout in seconds')
@click.option('--databases-only', is_flag=True, help='Start only database services')
@click.option('--quick', is_flag=True, help='Skip detailed health checks for faster startup')
@click.pass_context
def start(ctx, timeout, databases_only, quick):
    """üöÄ Start the multi-agent system with improved timeout handling"""

    manager: SystemManager = ctx.obj['manager']
    verbose = ctx.obj['verbose']

    echo_header("üöÄ STARTING MULTI-AGENT RESEARCH SYSTEM")

    try:
        # Step 1: Prerequisites
        if not manager.check_prerequisites():
            raise click.ClickException("Prerequisites check failed")

        # Step 2: Environment setup
        if not manager.setup_environment():
            raise click.ClickException("Environment setup failed")

        # Step 3: Check sources (info only)
        manager.check_sources()

        # Step 4: Start services
        echo_step("üê≥", "Starting services...")

        if databases_only:
            # Start only database services
            db_services = ["neo4j", "mongodb", "chromadb"]
            if not manager.run_compose_command(["up", "-d"] + db_services, timeout=timeout):
                echo_warning("Service startup may have timed out, but checking if services are running...")

            echo_success("Database services started")

            # Wait for database health (with shorter timeouts if quick mode)
            if not quick:
                health_timeout = 120
                for service in db_services:
                    if not manager.wait_for_service(service, max_wait=health_timeout):
                        echo_warning(f"Service {service} not ready, but may still be starting...")

            echo_success("Database services are running")
            show_startup_summary()
            return

        # Start all services
        echo_info(f"Starting all services (timeout: {timeout}s)...")
        if not manager.run_compose_command(["up", "-d"], timeout=timeout):
            echo_warning("Startup may have timed out, checking service status...")

        echo_success("Services started")

        # Health checks (skip if quick mode)
        if not quick:
            echo_step("üîç", "Performing health checks...")
            key_services = ["neo4j", "mongodb", "chromadb"]
            service_timeout = 120

            for service in key_services:
                if not manager.wait_for_service(service, max_wait=service_timeout):
                    echo_warning(f"Service {service} not ready, but may still be working...")

            # Check API if it exists
            if manager.wait_for_service("api", max_wait=120):
                echo_success("API service is ready")
            else:
                echo_warning("API service not ready, but may still be starting...")

        # Final verification with actual HTTP calls
        echo_step("üîç", "Verifying system...")
        if verify_system_with_http_calls(quick):
            echo_success("System verification completed")
        else:
            echo_warning("Some services may still be starting up...")

        # Show final status
        show_startup_summary()

        echo_header("‚úÖ SYSTEM STARTUP COMPLETE")
        echo_success("Your multi-agent research system is ready!")

        # Show next steps
        click.echo("\n" + click.style("üìã Quick Commands:", fg='cyan', bold=True))
        click.echo("   python cli.py test      # Test system functionality")
        click.echo("   python cli.py status    # Check detailed status")
        click.echo("   python cli.py health    # Run health checks")

    except click.ClickException:
        raise
    except KeyboardInterrupt:
        echo_warning("Startup interrupted by user")
        ctx.exit(1)
    except Exception as e:
        echo_error(f"Unexpected error during startup: {e}")
        if verbose:
            import traceback
            click.echo(traceback.format_exc())
        ctx.exit(1)


@cli.command()
@click.pass_context
def quick_start(ctx):
    """‚ö° Quick start with minimal health checks"""
    ctx.invoke(start, timeout=300, databases_only=False, quick=True)


@cli.command()
@click.option('--volumes', is_flag=True, help='Remove volumes too')
@click.confirmation_option(prompt='This will stop all services. Continue?')
@click.pass_context
def stop(ctx, volumes):
    """üõë Stop all system services"""

    manager: SystemManager = ctx.obj['manager']

    echo_step("üõë", "Stopping multi-agent system...")

    cmd = ["down"]
    if volumes:
        cmd.append("-v")
        echo_warning("Removing volumes - all data will be lost!")

    if manager.run_compose_command(cmd):
        echo_success("System stopped successfully")

        if volumes:
            echo_info("All data volumes removed")
    else:
        echo_error("Failed to stop system")
        ctx.exit(1)


@cli.command()
@click.option('--service', '-s', help='Restart specific service only')
@click.pass_context
def restart(ctx, service):
    """üîÑ Restart the system or specific service"""

    manager: SystemManager = ctx.obj['manager']

    if service:
        echo_step("üîÑ", f"Restarting {service}...")

        if manager.run_compose_command(["restart", service]):
            echo_success(f"{service} restarted successfully")
        else:
            echo_error(f"Failed to restart {service}")
            ctx.exit(1)
    else:
        echo_step("üîÑ", "Restarting entire system...")

        # Stop and start
        ctx.invoke(stop, volumes=False)
        time.sleep(3)
        ctx.invoke(start)


@cli.command()
@click.option('--format', 'output_format', type=click.Choice(['table', 'json']), default='table', help='Output format')
@click.pass_context
def status(ctx, output_format):
    """üìä Show system status"""

    manager: SystemManager = ctx.obj['manager']

    echo_step("üìä", "Getting system status...")

    try:
        result = subprocess.run(
            ["docker-compose", "ps"],
            cwd=manager.project_root,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            if output_format == 'json':
                # Simple status in JSON format
                status_data = {"status": "success", "output": result.stdout}
                click.echo(json.dumps(status_data, indent=2))
            else:
                echo_success("System status:")
                click.echo(result.stdout)
        else:
            echo_error("Failed to get system status")
            ctx.exit(1)

    except Exception as e:
        echo_error(f"Error getting status: {e}")
        ctx.exit(1)


@cli.command()
@click.option('--service', '-s', help='Show logs for specific service')
@click.option('--lines', '-n', default=50, help='Number of lines to show')
@click.option('--follow', '-f', is_flag=True, help='Follow log output')
@click.pass_context
def logs(ctx, service, lines, follow):
    """üìã Show service logs"""

    manager: SystemManager = ctx.obj['manager']

    if follow:
        echo_step("üìã", f"Following logs{' for ' + service if service else ''}...")
        echo_info("Press Ctrl+C to stop")

        cmd = ["logs", "-f"]
        if service:
            cmd.append(service)

        try:
            subprocess.run(
                ["docker-compose"] + cmd,
                cwd=manager.project_root
            )
        except KeyboardInterrupt:
            echo_info("Log following stopped")
    else:
        manager.show_logs(service, lines)


@cli.command()
@click.option('--query', '-q', default='machine learning applications in healthcare',
              help='Research query to demonstrate')
@click.option('--endpoint', default='agent/clean', help='API endpoint to use (agent, agent/clean, agent/formatted)')
@click.option('--show-json', is_flag=True, help='Show raw JSON response')
@click.option('--article-format', is_flag=True, help='Format output for article/blog inclusion')
@click.pass_context
def demo(ctx, query, endpoint, show_json, article_format):
    """üé¨ Run a demonstration of the multi-agent research system

    Perfect for articles, presentations, and showcasing capabilities.
    """
    if article_format:
        # Special formatting for articles and blogs
        demo_article_format(query, endpoint, show_json)
    else:
        # Regular demo format
        demo_regular_format(ctx, query, endpoint, show_json)


def demo_article_format(query, endpoint, show_json):
    """Format demo output specifically for articles and documentation"""

    print("\n" + "=" * 80)
    print("üéØ MULTI-AGENT RESEARCH SYSTEM DEMONSTRATION")
    print("=" * 80)

    print(f"\nüìã QUERY: {query}")
    print(f"üîó ENDPOINT: /api/v1/{endpoint}")
    print(f"‚è±Ô∏è  PROCESSING: Starting analysis...")

    try:
        import time
        start_time = time.time()

        # Make the API call
        url = f"http://localhost:8000/api/v1/{endpoint}"
        response = requests.post(url, json={"query": query}, timeout=120)

        end_time = time.time()
        processing_time = end_time - start_time

        if response.status_code == 200:
            result = response.json()

            print(f"‚úÖ COMPLETED: {processing_time:.1f} seconds")
            print("\n" + "=" * 80)

            if endpoint == 'agent/clean':
                display_clean_demo_output(result)
            elif endpoint == 'agent/formatted':
                display_formatted_demo_output(result)
            else:
                display_standard_demo_output(result)

            if show_json:
                print("\n" + "=" * 80)
                print("üìÑ RAW JSON RESPONSE:")
                print("=" * 80)
                print(json.dumps(result, indent=2))

        else:
            print(f"‚ùå ERROR: {response.status_code}")
            print(f"Response: {response.text}")

    except requests.exceptions.ConnectionError:
        print("‚ùå CONNECTION ERROR: Cannot connect to API")
        print("üí° SOLUTION: Run 'python cli.py start' first")
    except Exception as e:
        print(f"‚ùå ERROR: {e}")


def demo_regular_format(ctx, query, endpoint, show_json):
    """Regular demo format"""
    echo_step("üé¨", f"Running demo with query: '{query}'")

    try:
        url = f"http://localhost:8000/api/v1/{endpoint}"
        response = requests.post(url, json={"query": query}, timeout=120)

        if response.status_code == 200:
            result = response.json()

            echo_success("Demo completed successfully!")

            if endpoint == 'agent/clean':
                display_clean_demo_output(result)
            elif endpoint == 'agent/formatted':
                display_formatted_demo_output(result)
            else:
                display_standard_demo_output(result)

            if show_json:
                click.echo(f"\nüìÑ Raw JSON Response:")
                click.echo(json.dumps(result, indent=2))
        else:
            echo_error(f"Demo failed: {response.status_code}")

    except Exception as e:
        echo_error(f"Demo error: {e}")


def display_clean_demo_output(result):
    """Improved display function with better formatting"""
    analysis = result.get('analysis', '')

    # Better parsing and display
    if analysis:
        # Remove common formatting artifacts
        clean_analysis = analysis.replace('\\n', '\n')
        clean_analysis = re.sub(r'^#+\s*', '', clean_analysis, flags=re.MULTILINE)
        clean_analysis = re.sub(r'\*\*(.*?)\*\*', r'\1', clean_analysis)
        clean_analysis = re.sub(r'={10,}', '', clean_analysis)

        # Split into meaningful paragraphs
        paragraphs = [p.strip() for p in clean_analysis.split('\n\n') if p.strip() and len(p.strip()) > 20]

        # Filter out metadata lines
        content_paragraphs = []
        for p in paragraphs:
            if not any(skip in p.lower() for skip in ['query:', 'research analysis results', 'system performance']):
                content_paragraphs.append(p)

        print("üìä ANALYSIS RESULTS:")
        print("-" * 60)

        if content_paragraphs:
            for i, paragraph in enumerate(content_paragraphs[:4], 1):  # Show first 4 meaningful paragraphs
                # Wrap long paragraphs
                wrapped = textwrap.fill(paragraph, width=70, initial_indent="", subsequent_indent="   ")
                print(f"\n{i}. {wrapped}")

            if len(content_paragraphs) > 4:
                print(f"\n   ... [and {len(content_paragraphs) - 4} more sections]")
        else:
            # Fallback - show raw analysis but cleaned
            preview = clean_analysis[:300].strip()
            if len(clean_analysis) > 300:
                preview += "..."
            print(f"\n{preview}")
    else:
        print("üìä ANALYSIS RESULTS:")
        print("-" * 60)
        print("No analysis content received")

    # System info with better formatting
    system_info = result.get('system_info', {})
    print(f"\nüîß SYSTEM PERFORMANCE:")
    print(f"   ‚Ä¢ Relationship Analyst: {'‚úÖ Used' if system_info.get('relationship_analyst_used') else '‚ö™ Not Used'}")
    print(f"   ‚Ä¢ Theme Analyst: {'‚úÖ Used' if system_info.get('theme_analyst_used') else '‚ö™ Not Used'}")
    print(f"   ‚Ä¢ Database Usage: {system_info.get('database_usage', 'Unknown')}")
    print(f"   ‚Ä¢ Response Quality: {system_info.get('response_quality', 'Unknown')}")


def display_formatted_demo_output(result):
    """Display formatted demo output for presentations"""
    query_info = result.get('query', {})
    analysis = result.get('analysis', {})
    performance = result.get('system_performance', {})

    print("üìä STRUCTURED ANALYSIS RESULTS:")
    print("-" * 40)

    if analysis.get('summary'):
        print(f"\nüéØ SUMMARY:")
        print(f"   {analysis['summary'][:200]}...")

    if analysis.get('key_insights'):
        print(f"\nüí° KEY INSIGHTS:")
        for insight in analysis['key_insights'][:3]:
            print(f"   ‚Ä¢ {insight}")

    if analysis.get('recommendations'):
        print(f"\nüìã RECOMMENDATIONS:")
        print(f"   {analysis['recommendations'][:150]}...")

    # System performance
    specialists = performance.get('specialists_activated', {})
    print(f"\nüîß SYSTEM ACTIVATION:")
    for specialist, info in specialists.items():
        status = "‚úÖ Active" if info.get('used') else "‚ö™ Inactive"
        print(f"   ‚Ä¢ {specialist.replace('_', ' ').title()}: {status}")

    print(f"\nüìà PERFORMANCE METRICS:")
    print(f"   ‚Ä¢ Database Utilization: {performance.get('database_utilization', 'Unknown')}")
    print(f"   ‚Ä¢ Confidence Level: {performance.get('confidence_level', 'Unknown')}")


def display_standard_demo_output(result):
    """Display standard demo output"""
    message = result.get('message', '')

    # Clean up the message for better display
    clean_message = message.replace('\\n', '\n').replace('# üéØ', 'üéØ')

    # Show first 500 characters
    if len(clean_message) > 500:
        display_message = clean_message[:500] + "\n\n... [truncated for demo]"
    else:
        display_message = clean_message

    print("üìä ANALYSIS RESULTS:")
    print("-" * 40)
    print(display_message)

    # System health
    system_health = result.get('system_health', {})
    print(f"\nüîß SYSTEM HEALTH:")
    for key, value in system_health.items():
        clean_key = key.replace('_', ' ').title()
        print(f"   ‚Ä¢ {clean_key}: {value}")


@cli.command()
@click.option('--query', '-q', default='machine learning', help='Test query to send')
@click.option('--endpoint', default='agent', help='API endpoint to test')
@click.option('--timeout', default=60, help='Request timeout in seconds')
@click.option('--simple', '-s', is_flag=True, help='Show simple, clean output')
@click.pass_context
def test(ctx, query, endpoint, timeout, simple):
    """üß™ Test system functionality"""

    if simple:
        # Simple, clean output mode
        echo_step("üß™", f"Testing: '{query}'")

        try:
            url = f"http://localhost:8000/api/v1/{endpoint}"
            response = requests.post(url, json={"query": query}, timeout=timeout)

            if response.status_code == 200:
                result = response.json()
                message = result.get('message', '')
                system_health = result.get('system_health', {})

                # Clean output
                print(f"\n{'=' * 60}")
                print(f"üìã QUERY: {query}")
                print(f"{'=' * 60}")
                print(f"{message}")
                print(f"\n{'=' * 60}")
                print(f"üîß SYSTEM STATUS:")
                for key, value in system_health.items():
                    clean_key = key.replace('_', ' ').title()
                    print(f"   {clean_key}: {value}")
                print(f"{'=' * 60}")

            else:
                echo_error(f"Request failed: {response.status_code}")

        except requests.exceptions.Timeout:
            echo_error(f"Request timed out after {timeout}s")
        except requests.exceptions.ConnectionError:
            echo_error("Cannot connect to API - is the system running?")
        except Exception as e:
            echo_error(f"Error: {e}")

        return

    # Original detailed output mode
    echo_step("üß™", f"Testing {endpoint} endpoint with query: '{query}'")

    try:
        url = f"http://localhost:8000/api/v1/{endpoint}"

        if endpoint == 'agent':
            response = requests.post(
                url,
                json={"query": query},
                timeout=timeout
            )
        else:
            response = requests.get(url, timeout=timeout)

        if response.status_code == 200:
            echo_success(f"API test successful!")

            try:
                result = response.json()

                if endpoint == 'agent':
                    message = result.get('message', '')
                    system_health = result.get('system_health', {})

                    # Clean metrics display
                    click.echo(f"\nüìä {click.style('Test Results:', fg='cyan', bold=True)}")
                    click.echo(f"   Query: {click.style(query, fg='white', bold=True)}")
                    click.echo(f"   Response Length: {click.style(f'{len(message):,} characters', fg='green')}")

                    # System health
                    click.echo(f"\nüîß {click.style('System Health:', fg='cyan', bold=True)}")
                    for key, value in system_health.items():
                        clean_key = key.replace('_', ' ').title()
                        # Color code based on status
                        if '‚úÖ' in str(value):
                            color = 'green'
                        elif 'üü°' in str(value):
                            color = 'yellow'
                        elif '‚ùå' in str(value):
                            color = 'red'
                        else:
                            color = 'white'

                        click.echo(f"   {clean_key}: {click.style(str(value), fg=color)}")

                    # Show response preview
                    click.echo(f"\nüìÑ {click.style('Response Preview:', fg='cyan', bold=True)}")

                    # Show first 300 chars of clean message
                    preview = message[:300].replace('\n', ' ').strip()
                    if len(message) > 300:
                        preview += "..."
                    click.echo(f"   {preview}")

                else:
                    click.echo(f"\nüìÑ Response: {json.dumps(result, indent=2)}")

            except json.JSONDecodeError:
                click.echo(f"\nüìÑ Raw Response: {response.text}")

        else:
            echo_error(f"API test failed with status {response.status_code}")
            click.echo(f"Response: {response.text}")
            ctx.exit(1)

    except requests.exceptions.ConnectionError:
        echo_error("Cannot connect to API - is the system running?")
        echo_info("Try: python cli.py start")
        ctx.exit(1)
    except requests.exceptions.Timeout:
        echo_error(f"Request timed out after {timeout}s")
        ctx.exit(1)
    except Exception as e:
        echo_error(f"Test failed: {e}")
        ctx.exit(1)


@cli.command()
@click.option('--detailed', is_flag=True, help='Run detailed health checks')
@click.pass_context
def health(ctx, detailed):
    """üè• Run system health checks"""

    echo_step("üè•", "Running system health checks...")

    health_results = []

    # Check API
    echo_info("Checking API health...")
    try:
        response = requests.get("http://localhost:8000/api/v1/status", timeout=10)
        api_healthy = response.status_code == 200
    except:
        api_healthy = False

    health_results.append(("API", api_healthy))

    if api_healthy:
        echo_success("API is responding")
    else:
        echo_error("API is not responding")

    if detailed:
        # Check databases
        echo_info("Checking database connections...")

        # Neo4j
        try:
            response = requests.get("http://localhost:7474", timeout=10)
            neo4j_healthy = response.status_code == 200
            echo_success("Neo4j: Connected")
            health_results.append(("Neo4j", True))
        except Exception as e:
            echo_error(f"Neo4j: Failed")
            health_results.append(("Neo4j", False))

        # MongoDB
        try:
            from pymongo import MongoClient
            client = MongoClient("mongodb://user:password@localhost:27017/", serverSelectionTimeoutMS=5000)
            client.server_info()
            echo_success("MongoDB: Connected")
            health_results.append(("MongoDB", True))
        except Exception as e:
            echo_error(f"MongoDB: Failed")
            health_results.append(("MongoDB", False))

        # ChromaDB
        try:
            response = requests.get("http://localhost:8001", timeout=10)
            chroma_healthy = response.status_code == 200
            echo_success("ChromaDB: Connected")
            health_results.append(("ChromaDB", True))
        except Exception as e:
            echo_error(f"ChromaDB: Failed")
            health_results.append(("ChromaDB", False))

    # Summary
    healthy_count = sum(1 for _, healthy in health_results if healthy)
    total_checks = len(health_results)

    click.echo(f"\nüìä Health Summary: {healthy_count}/{total_checks} checks passed")

    if healthy_count == total_checks:
        echo_success("System is fully healthy! üéâ")
    elif healthy_count > 0:
        echo_warning("System is partially healthy - some components need attention")
        ctx.exit(1)
    else:
        echo_error("System is unhealthy - major issues detected")
        ctx.exit(1)


# Enhanced comparative analysis and demo commands

@cli.command()
@click.option('--show-content', is_flag=True, help='Show actual database content to guide queries')
@click.option('--suggest-queries', is_flag=True, help='Suggest queries based on database content')
@click.pass_context
def explore_data(ctx, show_content, suggest_queries):
    """üîç Explore database content to understand what queries will work

    Shows what's actually in your databases and suggests good queries.
    """
    echo_header("üîç DATABASE CONTENT EXPLORATION")

    # Check MongoDB content
    echo_step("1.", "Exploring MongoDB document content...")
    try:
        from pymongo import MongoClient
        client = MongoClient("mongodb://user:password@localhost:27017/", serverSelectionTimeoutMS=5000)
        db = client['research_db']

        papers_count = db.papers.count_documents({})
        topics_count = db.topics.count_documents({})

        if papers_count == 0:
            echo_warning("No papers in MongoDB - run ingestion first")
            return

        echo_success(f"Found {papers_count} papers and {topics_count} topic entries")

        if show_content:
            echo_info("\nüìÑ Sample Paper Titles:")
            sample_papers = db.papers.find({},
                                           {"metadata.title": 1, "metadata.authors": 1, "metadata.keywords": 1}).limit(
                5)
            for i, paper in enumerate(sample_papers, 1):
                title = paper.get("metadata", {}).get("title", "Unknown Title")
                authors = paper.get("metadata", {}).get("authors", [])
                keywords = paper.get("metadata", {}).get("keywords", [])

                echo_info(f"   {i}. {title[:60]}...")
                if authors:
                    echo_info(f"      Authors: {', '.join(authors[:2])}{'...' if len(authors) > 2 else ''}")
                if keywords:
                    echo_info(f"      Keywords: {', '.join(keywords[:3])}{'...' if len(keywords) > 3 else ''}")
                echo_info("")

        # Show topic categories
        echo_info("üìä Topic Categories Found:")
        topic_categories = db.topics.distinct("category")
        for category in topic_categories[:8]:
            # Count papers in this category
            count = db.topics.count_documents({"category": category})
            echo_info(f"   ‚Ä¢ {category}: {count} entries")

        if len(topic_categories) > 8:
            echo_info(f"   ‚Ä¢ ... and {len(topic_categories) - 8} more categories")

        # Show common keywords
        echo_info("\nüè∑Ô∏è Common Keywords:")
        all_keywords = []
        keyword_papers = db.papers.find({}, {"metadata.keywords": 1}).limit(20)
        for paper in keyword_papers:
            keywords = paper.get("metadata", {}).get("keywords", [])
            all_keywords.extend(keywords)

        if all_keywords:
            from collections import Counter
            common_keywords = Counter(all_keywords).most_common(10)
            for keyword, count in common_keywords:
                echo_info(f"   ‚Ä¢ {keyword}: appears in {count} papers")

    except Exception as e:
        echo_error(f"MongoDB exploration failed: {e}")
        return

    # Check Neo4j content
    echo_step("2.", "Exploring Neo4j graph content...")
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))

        with driver.session() as session:
            # Count different node types
            node_types = session.run("""
                MATCH (n) 
                RETURN labels(n) as labels, count(n) as count 
                ORDER BY count DESC
            """)

            echo_info("üï∏Ô∏è Graph Node Types:")
            concept_names = []
            author_names = []

            for record in node_types:
                labels = record["labels"]
                count = record["count"]
                if labels:
                    label = labels[0]
                    echo_info(f"   ‚Ä¢ {label}: {count} nodes")

                    # Collect sample names for suggestions
                    if label == "Concept" and show_content:
                        sample_concepts = session.run("MATCH (c:Concept) RETURN c.name as name LIMIT 5")
                        concept_names = [record["name"] for record in sample_concepts if record["name"]]
                    elif label == "Author" and show_content:
                        sample_authors = session.run("MATCH (a:Author) RETURN a.name as name LIMIT 5")
                        author_names = [record["name"] for record in sample_authors if record["name"]]

            if show_content and concept_names:
                echo_info("\nüß† Sample Concepts:")
                for concept in concept_names:
                    echo_info(f"   ‚Ä¢ {concept}")

            if show_content and author_names:
                echo_info("\nüë§ Sample Authors:")
                for author in author_names:
                    echo_info(f"   ‚Ä¢ {author}")

            # Show relationship types
            rel_types = session.run("""
                MATCH ()-[r]->() 
                RETURN type(r) as rel_type, count(r) as count 
                ORDER BY count DESC LIMIT 5
            """)

            echo_info("\nüîó Relationship Types:")
            for record in rel_types:
                rel_type = record["rel_type"]
                count = record["count"]
                echo_info(f"   ‚Ä¢ {rel_type}: {count} relationships")

    except Exception as e:
        echo_error(f"Neo4j exploration failed: {e}")

    # Generate suggested queries
    if suggest_queries:
        echo_step("3.", "Generating suggested queries based on your data...")

        suggested_queries = generate_smart_queries(topic_categories, concept_names, author_names, common_keywords)

        echo_info("üéØ Suggested Queries (based on your actual data):")
        for category, queries in suggested_queries.items():
            echo_info(f"\n   {category.upper()}:")
            for query in queries:
                echo_info(f"   ‚Ä¢ \"{query}\"")

    echo_info(f"\nüí° Next Steps:")
    echo_info("   1. Try the suggested queries above")
    echo_info("   2. Use: python cli.py test-query \"<your query>\"")
    echo_info("   3. Or: python cli.py demo --query \"<your query>\"")


def generate_smart_queries(topic_categories, concept_names, author_names, common_keywords):
    """Generate smart queries based on actual database content"""

    suggested_queries = {
        "topic_based": [],
        "concept_based": [],
        "author_based": [],
        "keyword_based": [],
        "relationship_based": []
    }

    # Topic-based queries
    if topic_categories:
        for category in topic_categories[:3]:
            suggested_queries["topic_based"].extend([
                f"What are the main themes in {category.lower()} research?",
                f"What trends are emerging in {category.lower()}?",
                f"What papers focus on {category.lower()}?"
            ])

    # Concept-based queries
    if concept_names:
        for concept in concept_names[:2]:
            suggested_queries["concept_based"].extend([
                f"How does {concept} relate to other research concepts?",
                f"What papers discuss {concept}?",
                f"Show me research connections involving {concept}"
            ])

    # Author-based queries
    if author_names:
        for author in author_names[:2]:
            suggested_queries["author_based"].extend([
                f"What research areas does {author} work in?",
                f"Who has collaborated with {author}?",
                f"What are {author}'s main research contributions?"
            ])

    # Keyword-based queries
    if common_keywords:
        top_keywords = [kw[0] for kw in common_keywords[:3]]
        for keyword in top_keywords:
            suggested_queries["keyword_based"].extend([
                f"What research involves {keyword}?",
                f"How is {keyword} used across different papers?",
                f"What are the applications of {keyword}?"
            ])

    # Relationship queries
    suggested_queries["relationship_based"] = [
        "What are the main research collaborations in this collection?",
        "How do different research concepts connect to each other?",
        "What are the citation patterns in these papers?",
        "Which authors work on similar topics?"
    ]

    return suggested_queries


@cli.command()
@click.argument('query')
@click.option('--endpoint', default='agent/clean', help='Endpoint to test')
@click.option('--explain', is_flag=True, help='Explain why query might not work')
@click.option('--verbose', '-v', is_flag=True, help='Show detailed response')
@click.pass_context
def test_query(ctx, query, endpoint, explain, verbose):
    """üß™ Test a specific query and analyze why it might not return good results

    Tests your query and explains what the agents are finding (or not finding).
    """
    echo_step("üß™", f"Testing query: \"{query}\"")

    if explain:
        analyze_query_potential(query)

    echo_info(f"Using endpoint: /api/v1/{endpoint}")
    echo_info("Processing...")

    try:
        start_time = time.time()

        url = f"http://localhost:8000/api/v1/{endpoint}"
        response = requests.post(url, json={"query": query}, timeout=120)

        end_time = time.time()
        processing_time = end_time - start_time

        if response.status_code == 200:
            result = response.json()

            echo_success(f"Query completed in {processing_time:.1f} seconds")

            # Analyze the response quality
            analyze_response_quality(result, query, verbose)

            if verbose:
                echo_info("\nüìÑ Full Response:")
                if endpoint == 'agent/clean':
                    analysis = result.get('analysis', '')
                    print(f"\n{analysis}")
                else:
                    print(json.dumps(result, indent=2))
        else:
            echo_error(f"Query failed: {response.status_code}")
            echo_info(f"Response: {response.text}")

    except Exception as e:
        echo_error(f"Query test failed: {e}")


def analyze_query_potential(query):
    """Analyze why a query might or might not work well"""
    echo_info("\nüîç Query Analysis:")

    # Check query characteristics
    words = query.lower().split()
    query_lower = query.lower()

    # Good indicators
    good_indicators = {
        'relationship_words': ['connect', 'relate', 'relationship', 'network', 'collaboration', 'citation'],
        'theme_words': ['theme', 'trend', 'pattern', 'topic', 'research', 'field', 'area'],
        'specific_terms': ['machine learning', 'neural network', 'deep learning', 'ai', 'algorithm'],
        'question_words': ['what', 'how', 'which', 'who', 'where', 'when', 'why']
    }

    found_indicators = []
    for category, indicators in good_indicators.items():
        for indicator in indicators:
            if indicator in query_lower:
                found_indicators.append((category, indicator))

    if found_indicators:
        echo_success("Query characteristics (good indicators found):")
        for category, indicator in found_indicators:
            category_name = category.replace('_', ' ').title()
            echo_info(f"   ‚Ä¢ {category_name}: '{indicator}'")
    else:
        echo_warning("Query might be too general or vague")
        echo_info("   ‚Ä¢ Try including specific research terms")
        echo_info("   ‚Ä¢ Use words like 'connect', 'relate', 'themes', 'trends'")
        echo_info("   ‚Ä¢ Be more specific about what you want to know")

    # Length analysis
    if len(words) < 3:
        echo_warning("Query is quite short - consider being more specific")
    elif len(words) > 15:
        echo_warning("Query is quite long - consider being more focused")
    else:
        echo_success("Query length is good")


def analyze_response_quality(result, query, verbose):
    """Analyze the quality of the response"""
    echo_info("\nüìä Response Quality Analysis:")

    # Check which specialists were used
    if 'system_info' in result:
        system_info = result['system_info']
        rel_used = system_info.get('relationship_analyst_used', False)
        theme_used = system_info.get('theme_analyst_used', False)
        db_usage = system_info.get('database_usage', 'Unknown')

        echo_info(f"   ‚Ä¢ Relationship Analyst: {'‚úÖ Used' if rel_used else '‚ùå Not Used'}")
        echo_info(f"   ‚Ä¢ Theme Analyst: {'‚úÖ Used' if theme_used else '‚ùå Not Used'}")
        echo_info(f"   ‚Ä¢ Database Usage: {db_usage}")

        # Provide feedback
        if not rel_used and not theme_used:
            echo_warning("Neither specialist was used effectively")
            echo_info("   üí° Your query might be too general")
            echo_info("   üí° Try: python cli.py explore-data --suggest-queries")
        elif 'üü° Partial' in db_usage:
            echo_warning("Partial database usage detected")
            echo_info("   üí° Query might not match your data well")
            echo_info("   üí° Try: python cli.py explore-data --show-content")
        elif '‚úÖ High' in db_usage:
            echo_success("Excellent! Query used databases effectively")

    # Check response content quality
    if 'analysis' in result:
        analysis = result['analysis']

        # Look for indicators of database-driven vs fallback responses
        fallback_indicators = [
            'not feasible to provide',
            'not possible to identify',
            'absence of relationship',
            'no meaningful search terms',
            'database may be empty',
            'general knowledge'
        ]

        has_fallback = any(indicator in analysis.lower() for indicator in fallback_indicators)

        if has_fallback:
            echo_warning("Response appears to be a fallback (not database-driven)")
            echo_info("   üí° Suggestions:")
            echo_info("     - Check if your query matches database content")
            echo_info("     - Try more specific research terms")
            echo_info("     - Run: python cli.py explore-data --suggest-queries")
        else:
            echo_success("Response appears to be database-driven")

    # Provide specific suggestions
    echo_info("\nüí° Improvement Suggestions:")
    query_lower = query.lower()

    if 'machine learning' in query_lower:
        echo_info("   ‚Ä¢ Try: 'How do machine learning techniques connect to computer vision?'")
        echo_info("   ‚Ä¢ Or: 'What are the main themes in machine learning research?'")
    elif 'ai' in query_lower or 'artificial intelligence' in query_lower:
        echo_info("   ‚Ä¢ Try: 'What research trends are emerging in artificial intelligence?'")
        echo_info("   ‚Ä¢ Or: 'How do AI techniques relate to specific application domains?'")
    else:
        echo_info("   ‚Ä¢ Be more specific about the research area")
        echo_info("   ‚Ä¢ Include relationship words: 'connect', 'relate', 'influence'")
        echo_info("   ‚Ä¢ Include analysis words: 'themes', 'trends', 'patterns'")


@cli.command()
@click.option('--source-dir', default='sources', help='Directory containing PDF files')
@click.option('--quick', is_flag=True, help='Process only first 2 PDFs for testing')
@click.pass_context
def ingest(ctx, source_dir, quick):
    """üìÑ Run data ingestion pipeline to populate databases

    Processes PDF files and populates all databases with research data.
    """
    echo_header("üìÑ DATA INGESTION PIPELINE")

    # Check prerequisites
    echo_step("1.", "Checking prerequisites...")

    sources_path = Path(source_dir)
    if not sources_path.exists():
        echo_error(f"Source directory {source_dir} not found")
        echo_info("Create the directory and add PDF files:")
        echo_info(f"  mkdir {source_dir}")
        echo_info(f"  # Copy your PDF files to {source_dir}/")
        return

    pdf_files = list(sources_path.glob("*.pdf"))
    if not pdf_files:
        echo_error(f"No PDF files found in {source_dir}")
        echo_info("Add PDF research papers to the sources directory")
        return

    echo_success(f"Found {len(pdf_files)} PDF files")

    if quick and len(pdf_files) > 2:
        echo_info(f"Quick mode: Processing first 2 files only")
        pdf_files = pdf_files[:2]

    # Check OpenAI API key
    import os
    if not os.getenv("OPENAI_API_KEY"):
        echo_error("OPENAI_API_KEY not set in environment")
        echo_info("Set your API key in .env file")
        return

    # Run ingestion
    echo_step("2.", "Starting ingestion process...")
    echo_info("This may take several minutes depending on the number of PDFs")

    try:
        # Import and run ingestion
        import sys
        sys.path.insert(0, 'src')

        from src.utils.ingestion_pipeline import PdfIngestionPipeline

        pipeline = PdfIngestionPipeline()

        successful = 0
        total = len(pdf_files)

        for i, pdf_file in enumerate(pdf_files, 1):
            echo_info(f"Processing {i}/{total}: {pdf_file.name}")

            try:
                if pipeline.process_pdf(pdf_file):
                    echo_success(f"‚úÖ {pdf_file.name}")
                    successful += 1
                else:
                    echo_warning(f"‚ö†Ô∏è {pdf_file.name} (partial/failed)")
            except Exception as e:
                echo_error(f"‚ùå {pdf_file.name}: {e}")

        # Test results
        echo_step("3.", "Testing ingestion results...")
        results = pipeline.test_ingestion_quality()

        echo_success(f"Ingestion completed: {successful}/{total} files processed")

        echo_info("Database content:")
        for db_name, stats in results.items():
            echo_info(f"  ‚Ä¢ {db_name.upper()}: {stats}")

        pipeline.close_connections()

        # Test the system
        echo_step("4.", "Testing system with sample query...")
        try:
            response = requests.post(
                "http://localhost:8000/api/v1/agent/clean",
                json={"query": "machine learning"},
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                system_info = result.get('system_info', {})
                db_usage = system_info.get('database_usage', 'Unknown')

                if 'High' in db_usage:
                    echo_success("‚úÖ System test passed - databases are working!")
                else:
                    echo_warning(f"‚ö†Ô∏è System test partial - database usage: {db_usage}")
            else:
                echo_warning("System test failed - check API status")

        except Exception as e:
            echo_warning(f"System test error: {e}")

        echo_header("‚úÖ INGESTION COMPLETE")
        echo_success("Your system is now ready for research queries!")
        echo_info("Try: python cli.py demo --article-format")

    except Exception as e:
        echo_error(f"Ingestion failed: {e}")
        echo_info("Check logs: python cli.py logs")


@cli.command()
@click.option('--domain', default='llm_critique', help='Research domain to analyze')
@click.option('--save-results', help='Save analysis results to file')
@click.option('--format', 'output_format', type=click.Choice(['detailed', 'summary', 'academic']), default='detailed')
@click.pass_context
def comparative_analysis(ctx, domain, save_results, output_format):
    """üî¨ Run sophisticated comparative analysis across research papers

    Demonstrates advanced research capabilities with complex comparative queries.
    Perfect for showcasing deep analytical capabilities in articles and presentations.
    """
    echo_header("üî¨ COMPARATIVE RESEARCH ANALYSIS")

    # Define sophisticated query templates by research domain
    query_templates = get_comparative_query_templates(domain)

    echo_info(f"üìä Analysis Domain: {domain.replace('_', ' ').title()}")
    echo_info(f"üéØ Query Categories: {len(query_templates)} analytical dimensions")
    echo_info(f"üìù Output Format: {output_format}")

    results = []

    for i, (category, queries) in enumerate(query_templates.items(), 1):
        echo_step(f"{i}.", f"Analyzing {category.replace('_', ' ').title()}")

        # Run the most sophisticated query from each category
        primary_query = queries[0]  # First query is usually the most comprehensive

        echo_info(f"Query: {primary_query[:80]}...")

        try:
            start_time = time.time()

            # Use the formatted endpoint for structured analysis
            url = "http://localhost:8000/api/v1/agent/formatted"
            response = requests.post(url, json={"query": primary_query}, timeout=180)

            end_time = time.time()
            processing_time = end_time - start_time

            if response.status_code == 200:
                result = response.json()

                # Enhanced result with meta-analysis
                enhanced_result = enhance_comparative_result(result, category, primary_query)
                results.append(enhanced_result)

                echo_success(f"Completed in {processing_time:.1f}s")

                # Display based on format
                if output_format == 'summary':
                    display_comparative_summary(enhanced_result)
                elif output_format == 'academic':
                    display_academic_format(enhanced_result)
                else:
                    display_detailed_comparative(enhanced_result)

            else:
                echo_error(f"Analysis failed: {response.status_code}")

        except Exception as e:
            echo_error(f"Error in {category}: {e}")

        if i < len(query_templates):
            echo_info("‚è≥ Moving to next analytical dimension...")
            time.sleep(2)

    # Generate comprehensive synthesis
    echo_step("üîÑ", "Synthesizing comparative insights...")
    synthesis = generate_comparative_synthesis(results, domain)

    echo_header("üìä COMPARATIVE ANALYSIS SYNTHESIS")
    display_synthesis(synthesis, output_format)

    # Save results if requested
    if save_results:
        save_comparative_analysis(results, synthesis, save_results, domain)
        echo_success(f"üíæ Analysis saved to: {save_results}")


def get_comparative_query_templates(domain):
    """Get sophisticated comparative query templates by research domain"""

    templates = {
        'llm_critique': {
            'thematic_analysis': [
                "How do the concepts of 'critique ability' and 'self-correction' compare across CRITIC, CRITICBENCH, and CritiqueLLM research papers?",
                "What are the shared theoretical foundations for LLM critique development across recent research papers?",
                "Compare how different papers conceptualize 'critique' as a capability, process, or training objective."
            ],
            'methodological_comparison': [
                "Compare the data collection and evaluation methodologies used in CRITICBENCH, CRITICEVAL, and AutoMathCritique papers.",
                "How do fine-tuning strategies differ between CTRL, MultiCritique, and CritiqueLLM in terms of supervision approaches?",
                "Analyze the differences in benchmark design across critique evaluation papers."
            ],
            'feedback_mechanisms': [
                "Which research approaches explore critique as iterative feedback versus static evaluation, and what are the implications?",
                "How does single-agent versus multi-agent critique generation influence quality outcomes across different studies?",
                "Compare the role of human feedback integration across critique research methodologies."
            ],
            'performance_claims': [
                "Which models demonstrate superior critique performance compared to GPT-4, and under what evaluation conditions?",
                "How consistent are reported improvements in reasoning tasks when critiques are introduced across different studies?",
                "Analyze the reproducibility and validity of performance claims across critique research papers."
            ],
            'training_vs_inference': [
                "Compare test-time critique application strategies in CRITIC, CGI, and MathCritique research.",
                "How do training-time critique strategies differ across RLHF and SFT approaches in recent papers?",
                "Analyze the trade-offs between training-time and inference-time critique integration."
            ],
            'theoretical_foundations': [
                "Which papers ground critique frameworks in cognitive science, and how is this operationalized?",
                "Compare human-in-the-loop methodologies across CRITICEVAL, MultiCritique, and CritiqueLLM studies.",
                "Analyze the philosophical assumptions about AI critique capability across different research approaches."
            ]
        },
        'transformer_architectures': {
            'attention_mechanisms': [
                "How do attention mechanisms evolve from vanilla Transformers through BERT, GPT, and T5 architectures?",
                "Compare multi-head attention strategies across different transformer variants and their impact on performance.",
                "Analyze the relationship between attention patterns and model interpretability across transformer research."
            ],
            'scaling_strategies': [
                "Compare parameter scaling approaches between GPT series, PaLM, and Chinchilla research lines.",
                "How do different papers approach the scaling laws for transformer architectures?",
                "Analyze computational efficiency strategies across large-scale transformer implementations."
            ],
            'architectural_innovations': [
                "Compare architectural modifications in Longformer, BigBird, and Linformer for handling long sequences.",
                "How do positional encoding strategies differ across transformer variants and their effectiveness?",
                "Analyze the trade-offs between architectural complexity and performance gains across transformer research."
            ]
        },
        'machine_learning_methods': {
            'learning_paradigms': [
                "Compare supervised, self-supervised, and reinforcement learning approaches across recent ML research.",
                "How do transfer learning strategies differ across computer vision and natural language processing domains?",
                "Analyze the evolution of few-shot learning methodologies across different research papers."
            ],
            'optimization_techniques': [
                "Compare optimization strategies and their convergence properties across different ML approaches.",
                "How do regularization techniques differ in effectiveness across various machine learning architectures?",
                "Analyze the relationship between optimization choices and generalization performance."
            ],
            'evaluation_frameworks': [
                "Compare evaluation methodologies and their validity across different machine learning research papers.",
                "How do benchmark designs differ across supervised and unsupervised learning research?",
                "Analyze the reproducibility challenges across different ML evaluation approaches."
            ]
        }
    }

    return templates.get(domain, templates['machine_learning_methods'])


def enhance_comparative_result(result, category, query):
    """Enhance result with comparative analysis metadata"""

    enhanced = {
        'category': category,
        'query': query,
        'original_result': result,
        'analysis_depth': assess_analysis_depth(result),
        'comparative_elements': extract_comparative_elements(result),
        'research_quality': assess_research_quality(result),
        'timestamp': time.time()
    }

    return enhanced


def assess_analysis_depth(result):
    """Assess the depth and quality of the analysis"""

    analysis = result.get('analysis', {})
    summary = analysis.get('summary', '')
    insights = analysis.get('key_insights', [])

    depth_score = 0
    depth_indicators = []

    # Check for comparative language
    comparative_terms = ['compare', 'contrast', 'differ', 'similar', 'versus', 'between', 'across']
    if any(term in summary.lower() for term in comparative_terms):
        depth_score += 2
        depth_indicators.append("Uses comparative analysis language")

    # Check for specific paper mentions
    if len(insights) > 2:
        depth_score += 2
        depth_indicators.append(f"Provides {len(insights)} key insights")

    # Check for methodological analysis
    methodology_terms = ['methodology', 'approach', 'framework', 'strategy', 'technique']
    if any(term in summary.lower() for term in methodology_terms):
        depth_score += 1
        depth_indicators.append("Includes methodological analysis")

    # Calculate the appropriate level based on score
    if depth_score >= 5:
        level = "Deep"
    elif depth_score >= 3:
        level = "Intermediate"
    elif depth_score >= 1:
        level = "Basic"
    else:
        level = "Surface"

    return {
        'score': depth_score,
        'level': level,
        'indicators': depth_indicators
    }


def extract_comparative_elements(result):
    """Extract comparative elements from the analysis"""

    analysis = result.get('analysis', {})
    summary = analysis.get('summary', '')
    insights = analysis.get('key_insights', [])

    comparative_elements = {
        'papers_mentioned': [],
        'methodologies_compared': [],
        'frameworks_analyzed': [],
        'performance_comparisons': []
    }

    # Extract paper names (simplified pattern matching)
    import re
    paper_patterns = [
        r'CRITIC\w*',
        r'BERT\w*',
        r'GPT-?\d*',
        r'T5\w*',
        r'MultiCritique',
        r'CritiqueLLM',
        r'AutoMathCritique'
    ]

    full_text = f"{summary} {' '.join(insights)}"

    for pattern in paper_patterns:
        matches = re.findall(pattern, full_text, re.IGNORECASE)
        comparative_elements['papers_mentioned'].extend(list(set(matches)))

    # Extract methodologies
    methodology_terms = ['fine-tuning', 'RLHF', 'SFT', 'attention', 'transformer', 'critique']
    for term in methodology_terms:
        if term.lower() in full_text.lower():
            comparative_elements['methodologies_compared'].append(term)

    return comparative_elements


def assess_research_quality(result):
    """Assess the research quality of the analysis"""

    system_performance = result.get('system_performance', {})
    specialists = system_performance.get('specialists_activated', {})

    quality_metrics = {
        'database_utilization': system_performance.get('database_utilization', 'Unknown'),
        'confidence_level': system_performance.get('confidence_level', 'Unknown'),
        'specialists_used': sum(1 for spec in specialists.values() if spec.get('used', False)),
        'total_specialists': len(specialists)
    }

    # Calculate quality score
    quality_score = 0
    if 'High' in quality_metrics['database_utilization']:
        quality_score += 3
    elif 'Partial' in quality_metrics['database_utilization']:
        quality_score += 1

    if quality_metrics['specialists_used'] == quality_metrics['total_specialists']:
        quality_score += 2
    elif quality_metrics['specialists_used'] > 0:
        quality_score += 1

    # Calculate quality level
    if quality_score >= 5:
        quality_level = "Excellent"
    elif quality_score >= 3:
        quality_level = "High"
    elif quality_score >= 1:
        quality_level = "Moderate"
    else:
        quality_level = "Low"

    quality_metrics['overall_score'] = quality_score
    quality_metrics['quality_level'] = quality_level

    return quality_metrics


def display_comparative_summary(result):
    """Display a concise summary of comparative analysis"""

    category = result['category'].replace('_', ' ').title()
    depth = result['analysis_depth']
    quality = result['research_quality']

    print(f"\nüìä {category} Analysis:")
    print(f"   ‚Ä¢ Depth Level: {depth['level']} ({depth['score']}/5)")
    print(f"   ‚Ä¢ Quality: {quality['quality_level']} ({quality['overall_score']}/5)")
    print(f"   ‚Ä¢ Database Usage: {quality['database_utilization']}")

    # Show key comparative elements
    elements = result['comparative_elements']
    if elements['papers_mentioned']:
        print(
            f"   ‚Ä¢ Papers Referenced: {', '.join(elements['papers_mentioned'][:3])}{'...' if len(elements['papers_mentioned']) > 3 else ''}")


def display_academic_format(result):
    """Display in academic paper format"""

    category = result['category'].replace('_', ' ').title()
    analysis = result['original_result'].get('analysis', {})

    print(f"\n## {category}")
    print(f"\n**Research Question:** {result['query'][:100]}...")

    if analysis.get('summary'):
        print(f"\n**Findings:** {analysis['summary'][:200]}...")

    if analysis.get('key_insights'):
        print(f"\n**Key Insights:**")
        for insight in analysis['key_insights'][:3]:
            print(f"‚Ä¢ {insight}")

    # Add methodological note
    quality = result['research_quality']
    print(
        f"\n**Methodology:** Multi-agent analysis with {quality['specialists_used']}/{quality['total_specialists']} specialists activated.")


def display_detailed_comparative(result):
    """Display detailed comparative analysis"""

    category = result['category'].replace('_', ' ').title()
    analysis = result['original_result'].get('analysis', {})
    depth = result['analysis_depth']
    elements = result['comparative_elements']

    print(f"\n{'=' * 60}")
    print(f"üìä {category.upper()} ANALYSIS")
    print(f"{'=' * 60}")

    print(f"\nüéØ Query: {result['query']}")

    if analysis.get('summary'):
        print(f"\nüìã Summary:")
        print(f"   {analysis['summary']}")

    if analysis.get('key_insights'):
        print(f"\nüí° Key Insights:")
        for i, insight in enumerate(analysis['key_insights'], 1):
            print(f"   {i}. {insight}")

    # Comparative elements
    print(f"\nüîç Comparative Elements Identified:")
    if elements['papers_mentioned']:
        print(f"   ‚Ä¢ Papers: {', '.join(elements['papers_mentioned'])}")
    if elements['methodologies_compared']:
        print(f"   ‚Ä¢ Methods: {', '.join(elements['methodologies_compared'])}")

    # Analysis quality
    print(f"\nüìà Analysis Quality:")
    print(f"   ‚Ä¢ Depth: {depth['level']} (Score: {depth['score']}/5)")
    for indicator in depth['indicators']:
        print(f"     - {indicator}")


def generate_comparative_synthesis(results, domain):
    """Generate a synthesis across all comparative analyses"""

    if not results:
        return {"summary": "No analysis results to synthesize"}

    # Aggregate insights
    all_insights = []
    all_papers = []
    all_methodologies = []

    for result in results:
        analysis = result['original_result'].get('analysis', {})
        insights = analysis.get('key_insights', [])
        all_insights.extend(insights)

        elements = result['comparative_elements']
        all_papers.extend(elements['papers_mentioned'])
        all_methodologies.extend(elements['methodologies_compared'])

    # Remove duplicates
    unique_papers = list(set(all_papers))
    unique_methods = list(set(all_methodologies))

    # Calculate overall quality
    avg_depth = sum(r['analysis_depth']['score'] for r in results) / len(results) if results else 0
    avg_quality = sum(r['research_quality']['overall_score'] for r in results) / len(results) if results else 0

    synthesis = {
        'domain': domain,
        'total_analyses': len(results),
        'unique_papers_referenced': unique_papers,
        'methodologies_covered': unique_methods,
        'average_depth_score': round(avg_depth, 1),
        'average_quality_score': round(avg_quality, 1),
        'key_comparative_insights': all_insights[:5],  # Top 5 insights
        'research_coverage': assess_research_coverage(results)
    }

    return synthesis


def assess_research_coverage(results):
    """Assess how comprehensively the research domain was covered"""

    categories_analyzed = [r['category'] for r in results]

    coverage = {
        'analytical_dimensions': len(set(categories_analyzed)),
        'depth_distribution': {},
        'quality_distribution': {}
    }

    # Distribution analysis
    for result in results:
        depth_level = result['analysis_depth']['level']
        quality_level = result['research_quality']['quality_level']

        coverage['depth_distribution'][depth_level] = coverage['depth_distribution'].get(depth_level, 0) + 1
        coverage['quality_distribution'][quality_level] = coverage['quality_distribution'].get(quality_level, 0) + 1

    return coverage


def display_synthesis(synthesis, output_format):
    """Display the comparative analysis synthesis"""

    if output_format == 'academic':
        display_academic_synthesis(synthesis)
    elif output_format == 'summary':
        display_summary_synthesis(synthesis)
    else:
        display_detailed_synthesis(synthesis)


def display_detailed_synthesis(synthesis):
    """Display detailed synthesis"""

    print(f"üî¨ COMPARATIVE RESEARCH SYNTHESIS")
    print(f"{'=' * 50}")

    print(f"\nüìä Analysis Overview:")
    print(f"   ‚Ä¢ Domain: {synthesis['domain'].replace('_', ' ').title()}")
    print(f"   ‚Ä¢ Analyses Completed: {synthesis['total_analyses']}")
    print(f"   ‚Ä¢ Analytical Dimensions: {synthesis['research_coverage']['analytical_dimensions']}")

    print(f"\nüìö Research Coverage:")
    if synthesis['unique_papers_referenced']:
        print(
            f"   ‚Ä¢ Papers Referenced: {', '.join(synthesis['unique_papers_referenced'][:5])}{'...' if len(synthesis['unique_papers_referenced']) > 5 else ''}")
    if synthesis['methodologies_covered']:
        print(
            f"   ‚Ä¢ Methodologies: {', '.join(synthesis['methodologies_covered'][:5])}{'...' if len(synthesis['methodologies_covered']) > 5 else ''}")

    print(f"\nüìà Analysis Quality:")
    print(f"   ‚Ä¢ Average Depth Score: {synthesis['average_depth_score']}/5")
    print(f"   ‚Ä¢ Average Quality Score: {synthesis['average_quality_score']}/5")

    if synthesis['key_comparative_insights']:
        print(f"\nüí° Cross-Analysis Insights:")
        for i, insight in enumerate(synthesis['key_comparative_insights'], 1):
            print(f"   {i}. {insight[:80]}{'...' if len(insight) > 80 else ''}")


def display_academic_synthesis(synthesis):
    """Display synthesis in academic format"""

    print(f"\n# Comparative Analysis of {synthesis['domain'].replace('_', ' ').title()} Research")

    print(f"\n## Methodology")
    print(f"Multi-dimensional comparative analysis across {synthesis['total_analyses']} analytical frameworks, "
          f"covering {synthesis['research_coverage']['analytical_dimensions']} distinct research dimensions.")

    print(f"\n## Coverage")
    print(f"Analysis incorporated {len(synthesis['unique_papers_referenced'])} research papers and "
          f"{len(synthesis['methodologies_covered'])} distinct methodological approaches.")

    print(f"\n## Quality Assessment")
    print(f"Average analysis depth: {synthesis['average_depth_score']}/5.0")
    print(f"Average research quality: {synthesis['average_quality_score']}/5.0")


def display_summary_synthesis(synthesis):
    """Display concise synthesis summary"""

    print(f"\nüéØ {synthesis['domain'].replace('_', ' ').title()} Research Summary:")
    print(f"   ‚Ä¢ {synthesis['total_analyses']} comparative analyses completed")
    print(f"   ‚Ä¢ {len(synthesis['unique_papers_referenced'])} papers referenced")
    print(f"   ‚Ä¢ Quality: {synthesis['average_quality_score']}/5 | Depth: {synthesis['average_depth_score']}/5")


def save_comparative_analysis(results, synthesis, filename, domain):
    """Save comprehensive analysis results"""

    output_data = {
        'analysis_metadata': {
            'domain': domain,
            'timestamp': time.time(),
            'total_analyses': len(results),
            'analysis_framework': 'Multi-Agent Comparative Research System'
        },
        'individual_analyses': results,
        'synthesis': synthesis,
        'recommendations': generate_research_recommendations(synthesis)
    }

    import json
    with open(filename, 'w') as f:
        json.dump(output_data, f, indent=2, default=str)


def generate_research_recommendations(synthesis):
    """Generate research recommendations based on analysis"""

    recommendations = []

    if synthesis['average_depth_score'] < 3:
        recommendations.append("Consider adding more specific research papers to increase analysis depth")

    if synthesis['average_quality_score'] < 3:
        recommendations.append("Improve database coverage for higher quality comparative analysis")

    if len(synthesis['unique_papers_referenced']) < 5:
        recommendations.append("Expand paper collection to enable richer comparative insights")

    recommendations.extend([
        "Explore cross-domain comparative analysis opportunities",
        "Consider longitudinal analysis of research evolution",
        "Investigate collaboration patterns between referenced authors"
    ])

    return recommendations


@cli.command()
@click.option('--category', type=click.Choice(
    ['thematic', 'methodological', 'feedback', 'performance', 'training', 'theoretical', 'all']), default='all')
@click.option('--output-style', type=click.Choice(['article', 'academic', 'presentation']), default='article')
@click.option('--save-analysis', help='Save detailed analysis to file')
@click.pass_context
def critique_research_demo(ctx, category, output_style, save_analysis):
    """üéì Demonstrate advanced LLM critique research analysis

    Showcases sophisticated comparative analysis using your specific research queries.
    Perfect for articles about AI research analysis capabilities.
    """
    echo_header("üéì LLM CRITIQUE RESEARCH ANALYSIS DEMO")

    # Your specific sophisticated queries
    critique_queries = {
        'thematic': [
            "How do the concepts of 'critique ability' and 'self-correction' compare between CRITIC (Gou et al.) and CRITICBENCH (Lin et al.)?",
            "What are the shared assumptions about the role of external feedback in LLM critique development across CRITIC, CritiqueLLM, and CRITICEVAL?",
            "Compare how different papers conceptualize 'critique' as a capability, process, or training target."
        ],
        'methodological': [
            "Compare the data collection methodologies used in CRITICBENCH, CRITICEVAL, and AutoMathCritique. How do these affect the evaluation of LLM critique ability?",
            "How do the fine-tuning strategies differ between CTRL, MultiCritique, and CritiqueLLM, especially in terms of model supervision and critique fidelity?"
        ],
        'feedback': [
            "Which papers explore critique as a feedback loop versus a static evaluation step? What implications do their models have for LLM self-improvement?",
            "How does the reliance on single-agent vs multi-agent critique generation influence the final critique quality as reported in MultiCritique and CRITICEVAL?"
        ],
        'performance': [
            "Which models are reported to outperform GPT-4 as critics, and under what evaluation settings (e.g., reference-free, pairwise comparison)?",
            "How consistent are the reported improvements in reasoning tasks (e.g., math, coding) when critiques are introduced, as per AutoMathCritique, CRITICBENCH, and CTRL?"
        ],
        'training': [
            "Which approaches, such as CRITIC, CGI, and MathCritique, apply critique models at test-time, and how does this impact model performance?",
            "How do training-time critique strategies (e.g., RLHF, SFT with critique data) differ across papers like CTRL, MultiCritique, and CritiqueLLM?"
        ],
        'theoretical': [
            "Which papers ground their critique frameworks in human cognitive processes (e.g., critical thinking, meta-cognition), and how is this operationalized?",
            "Discuss how human-in-the-loop methodologies differ in rigor and purpose between CRITICEVAL, MultiCritique, and CritiqueLLM."
        ]
    }

    # Select queries based on category
    if category == 'all':
        selected_queries = []
        for cat_queries in critique_queries.values():
            selected_queries.append(cat_queries[0])  # First query from each category
    else:
        selected_queries = critique_queries.get(category, [])

    echo_info(f"üìä Analysis Category: {category.title() if category != 'all' else 'Comprehensive'}")
    echo_info(f"üéØ Queries to Process: {len(selected_queries)}")
    echo_info(f"üìù Output Style: {output_style}")

    if output_style == 'article':
        demo_for_article(selected_queries, category)
    elif output_style == 'academic':
        demo_for_academic(selected_queries, category)
    else:
        demo_for_presentation(selected_queries, category)

    if save_analysis:
        echo_info(f"üíæ Analysis will be saved to: {save_analysis}")


def demo_for_article(queries, category):
    """Format demo specifically for article inclusion"""

    print("\n" + "=" * 80)
    print("üéì ADVANCED AI RESEARCH ANALYSIS DEMONSTRATION")
    print("Multi-Agent System for Comparative Academic Research")
    print("=" * 80)

    print(f"\nüìã Research Focus: LLM Critique Research Analysis")
    print(f"üîç Analysis Type: {category.title() if category != 'all' else 'Multi-Dimensional Comparative'}")
    print(f"ü§ñ System: Multi-Agent Research Coordinator with Specialized Analysts")

    results = []

    for i, query in enumerate(queries, 1):
        print(f"\n{'‚Äî' * 70}")
        print(f"üéØ ANALYSIS {i}/{len(queries)}")
        print(f"{'‚Äî' * 70}")

        # Show the sophisticated query
        print(f"\nüìù Research Question:")
        print(f'   "{query}"')

        print(f"\n‚è≥ Processing with multi-agent system...")

        try:
            start_time = time.time()

            # Use the clean endpoint for article-friendly output
            url = "http://localhost:8000/api/v1/agent/clean"
            response = requests.post(url, json={"query": query}, timeout=180)

            end_time = time.time()
            processing_time = end_time - start_time

            if response.status_code == 200:
                result = response.json()
                results.append(result)

                print(f"‚úÖ Analysis completed in {processing_time:.1f} seconds")

                # Display the analysis in article format
                display_article_analysis(result, i)

            else:
                print(f"‚ùå Analysis failed: {response.status_code}")

        except Exception as e:
            print(f"‚ùå Error: {e}")

        if i < len(queries):
            print(f"\n‚è≥ Proceeding to next analysis...")
            time.sleep(2)

    # Article summary
    display_article_summary(results, category)


def display_article_analysis(result, analysis_number):
    """Display analysis in article-friendly format"""

    analysis = result.get('analysis', '').strip()
    system_info = result.get('system_info', {})

    print(f"\nüìä ANALYSIS RESULTS:")
    print(f"‚îÅ" * 50)

    # Clean and format the analysis for articles
    if analysis:
        # Remove excessive formatting
        clean_analysis = analysis.replace('Research Analysis Results', 'Key Findings')
        clean_analysis = clean_analysis.replace('===', '‚Äî')

        # Extract meaningful content
        paragraphs = [p.strip() for p in clean_analysis.split('\n\n') if p.strip() and len(p.strip()) > 30]

        # Show the most substantive paragraphs
        content_paragraphs = []
        for p in paragraphs:
            # Filter out metadata and system info
            if not any(skip in p.lower() for skip in ['query:', 'system performance', 'specialists used']):
                content_paragraphs.append(p)

        if content_paragraphs:
            for j, paragraph in enumerate(content_paragraphs[:2], 1):  # Show top 2 paragraphs
                wrapped = textwrap.fill(paragraph, width=75, initial_indent="   ", subsequent_indent="   ")
                print(f"\n{wrapped}")
        else:
            # Fallback display
            preview = analysis[:400].strip()
            if len(analysis) > 400:
                preview += "..."
            wrapped = textwrap.fill(preview, width=75, initial_indent="   ", subsequent_indent="   ")
            print(f"\n{wrapped}")

    # System performance summary
    rel_used = system_info.get('relationship_analyst_used', False)
    theme_used = system_info.get('theme_analyst_used', False)
    db_usage = system_info.get('database_usage', 'Unknown')

    print(f"\nüîß SYSTEM PERFORMANCE:")
    print(
        f"   ‚Ä¢ Multi-Agent Coordination: {'‚úÖ Full' if (rel_used and theme_used) else 'üü° Partial' if (rel_used or theme_used) else '‚ùå Limited'}")
    print(f"   ‚Ä¢ Database Integration: {db_usage}")
    print(f"   ‚Ä¢ Analysis Quality: {system_info.get('response_quality', 'Unknown')}")


def display_article_summary(results, category):
    """Display summary for article conclusion"""

    print(f"\n{'‚ïê' * 80}")
    print("üìä DEMONSTRATION SUMMARY")
    print(f"{'‚ïê' * 80}")

    if not results:
        print("‚ö†Ô∏è No successful analyses to summarize")
        return

    # Calculate metrics
    total_analyses = len(results)
    successful_analyses = len([r for r in results if r.get('analysis')])

    # System utilization
    rel_activations = sum(1 for r in results if r.get('system_info', {}).get('relationship_analyst_used', False))
    theme_activations = sum(1 for r in results if r.get('system_info', {}).get('theme_analyst_used', False))

    # Database usage quality
    high_usage = sum(1 for r in results if '‚úÖ High' in r.get('system_info', {}).get('database_usage', ''))
    partial_usage = sum(1 for r in results if 'üü° Partial' in r.get('system_info', {}).get('database_usage', ''))

    print(f"\nüéØ ANALYSIS OVERVIEW:")
    print(f"   ‚Ä¢ Research Domain: LLM Critique Research")
    print(f"   ‚Ä¢ Analytical Approach: {category.title() if category != 'all' else 'Multi-Dimensional Comparative'}")
    print(f"   ‚Ä¢ Queries Processed: {successful_analyses}/{total_analyses}")

    print(f"\nü§ñ MULTI-AGENT SYSTEM PERFORMANCE:")
    print(
        f"   ‚Ä¢ Relationship Analyst Activations: {rel_activations}/{total_analyses} ({rel_activations / total_analyses * 100:.0f}%)")
    print(
        f"   ‚Ä¢ Theme Analyst Activations: {theme_activations}/{total_analyses} ({theme_activations / total_analyses * 100:.0f}%)")
    print(f"   ‚Ä¢ High Database Utilization: {high_usage}/{total_analyses} ({high_usage / total_analyses * 100:.0f}%)")

    print(f"\nüìà RESEARCH CAPABILITIES DEMONSTRATED:")
    print(f"   ‚úÖ Sophisticated comparative analysis across multiple research papers")
    print(f"   ‚úÖ Multi-dimensional research question processing")
    print(f"   ‚úÖ Integration of relationship and thematic analysis")
    print(f"   ‚úÖ Real-time academic knowledge synthesis")

    print(f"\nüí° KEY INSIGHTS:")
    print(f"   ‚Ä¢ The system successfully processes complex comparative research queries")
    print(f"   ‚Ä¢ Multi-agent coordination enables nuanced academic analysis")
    print(f"   ‚Ä¢ Database integration provides evidence-based research insights")
    print(f"   ‚Ä¢ Processing time: 15-45 seconds per sophisticated query")

    print(f"\nüî¨ RESEARCH APPLICATIONS:")
    print(f"   ‚Ä¢ Literature review automation and synthesis")
    print(f"   ‚Ä¢ Comparative methodology analysis across research papers")
    print(f"   ‚Ä¢ Research gap identification and trend analysis")
    print(f"   ‚Ä¢ Academic collaboration and citation network mapping")


def demo_for_academic(queries, category):
    """Format demo for academic presentations"""

    print(f"\n# Multi-Agent Research Analysis: LLM Critique Research")
    print(f"\n## Methodology")
    print(f"Comparative analysis using multi-agent system with specialized research analysts.")
    print(f"Analysis category: {category.title()}")

    for i, query in enumerate(queries, 1):
        print(f"\n## Analysis {i}: {category.title()} Dimension")
        print(f"\n**Research Question:** {query}")

        # Process and display in academic format
        # [Implementation similar to above but with academic formatting]


def demo_for_presentation(queries, category):
    """Format demo for live presentations"""

    print(f"\nüéØ LIVE DEMO: Advanced Research Analysis")
    print(f"üìä Category: {category.title()}")
    print(f"ü§ñ System: Multi-Agent Research Coordinator")

    for i, query in enumerate(queries, 1):
        print(f"\n{'‚ñ∂' * 3} DEMO {i}: Processing sophisticated research query...")
        print(f"Query: {query[:60]}...")

        # Live processing with visual feedback
        # [Implementation with progress indicators for presentations]


@cli.command()
@click.pass_context
def article_demo(ctx):
    """üìù Complete demo ready for article inclusion

    Runs the full sophisticated analysis demo formatted for articles and blogs.
    """
    echo_header("üìù ARTICLE-READY RESEARCH DEMONSTRATION")

    echo_info("This demo showcases advanced comparative research analysis capabilities")
    echo_info("Perfect for inclusion in articles about AI research analysis systems")

    ctx.invoke(critique_research_demo, category='all', output_style='article')

    echo_header("‚úÖ ARTICLE DEMO COMPLETE")
    echo_success("Copy the above output directly into your article!")
    echo_info("üí° The demo shows sophisticated multi-agent research analysis in action")


if __name__ == '__main__':
    cli()
